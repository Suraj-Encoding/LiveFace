{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lEtjE8jDRWm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddab6256-7580-47ba-b822-7fcc6365d33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8jWVQDAsVUe",
        "outputId": "87744f5d-6a60-4438-fccf-b043055f18b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC-igHqJRRRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fec680-c7da-4d54-d004-4398b42b9d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Found 9123 images belonging to 2 classes.\n",
            "Found 3491 images belonging to 2 classes.\n",
            "\n",
            "Creating and training models...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Extracting CNN features from training data...\n",
            "Processed 2/286 batches\n",
            "Processed 4/286 batches\n",
            "Processed 6/286 batches\n",
            "Processed 8/286 batches\n",
            "Processed 10/286 batches\n",
            "Processed 12/286 batches\n",
            "Processed 14/286 batches\n",
            "Processed 16/286 batches\n",
            "Processed 18/286 batches\n",
            "Processed 20/286 batches\n",
            "Processed 22/286 batches\n",
            "Processed 24/286 batches\n",
            "Processed 26/286 batches\n",
            "Processed 28/286 batches\n",
            "Processed 30/286 batches\n",
            "Processed 32/286 batches\n",
            "Processed 34/286 batches\n",
            "Processed 36/286 batches\n",
            "Processed 38/286 batches\n",
            "Processed 40/286 batches\n",
            "Processed 42/286 batches\n",
            "Processed 44/286 batches\n",
            "Processed 46/286 batches\n",
            "Processed 48/286 batches\n",
            "Processed 50/286 batches\n",
            "Processed 52/286 batches\n",
            "Processed 54/286 batches\n",
            "Processed 56/286 batches\n",
            "Processed 58/286 batches\n",
            "Processed 60/286 batches\n",
            "Processed 62/286 batches\n",
            "Processed 64/286 batches\n",
            "Processed 66/286 batches\n",
            "Processed 68/286 batches\n",
            "Processed 70/286 batches\n",
            "Processed 72/286 batches\n",
            "Processed 74/286 batches\n",
            "Processed 76/286 batches\n",
            "Processed 78/286 batches\n",
            "Processed 80/286 batches\n",
            "Processed 82/286 batches\n",
            "Processed 84/286 batches\n",
            "Processed 86/286 batches\n",
            "Processed 88/286 batches\n",
            "Processed 90/286 batches\n",
            "Processed 92/286 batches\n",
            "Processed 94/286 batches\n",
            "Processed 96/286 batches\n",
            "Processed 98/286 batches\n",
            "Processed 100/286 batches\n",
            "Processed 102/286 batches\n",
            "Processed 104/286 batches\n",
            "Processed 106/286 batches\n",
            "Processed 108/286 batches\n",
            "Processed 110/286 batches\n",
            "Processed 112/286 batches\n",
            "Processed 114/286 batches\n",
            "Processed 116/286 batches\n",
            "Processed 118/286 batches\n",
            "Processed 120/286 batches\n",
            "Processed 122/286 batches\n",
            "Processed 124/286 batches\n",
            "Processed 126/286 batches\n",
            "Processed 128/286 batches\n",
            "Processed 130/286 batches\n",
            "Processed 132/286 batches\n",
            "Processed 134/286 batches\n",
            "Processed 136/286 batches\n",
            "Processed 138/286 batches\n",
            "Processed 140/286 batches\n",
            "Processed 142/286 batches\n",
            "Processed 144/286 batches\n",
            "Processed 146/286 batches\n",
            "Processed 148/286 batches\n",
            "Processed 150/286 batches\n",
            "Processed 152/286 batches\n",
            "Processed 154/286 batches\n",
            "Processed 156/286 batches\n",
            "Processed 158/286 batches\n",
            "Processed 160/286 batches\n",
            "Processed 162/286 batches\n",
            "Processed 164/286 batches\n",
            "Processed 166/286 batches\n",
            "Processed 168/286 batches\n",
            "Processed 170/286 batches\n",
            "Processed 172/286 batches\n",
            "Processed 174/286 batches\n",
            "Processed 176/286 batches\n",
            "Processed 178/286 batches\n",
            "Processed 180/286 batches\n",
            "Processed 182/286 batches\n",
            "Processed 184/286 batches\n",
            "Processed 186/286 batches\n",
            "Processed 188/286 batches\n",
            "Processed 190/286 batches\n",
            "Processed 192/286 batches\n",
            "Processed 194/286 batches\n",
            "Processed 196/286 batches\n",
            "Processed 198/286 batches\n",
            "Processed 200/286 batches\n",
            "Processed 202/286 batches\n",
            "Processed 204/286 batches\n",
            "Processed 206/286 batches\n",
            "Processed 208/286 batches\n",
            "Processed 210/286 batches\n",
            "Processed 212/286 batches\n",
            "Processed 214/286 batches\n",
            "Processed 216/286 batches\n",
            "Processed 218/286 batches\n",
            "Processed 220/286 batches\n",
            "Processed 222/286 batches\n",
            "Processed 224/286 batches\n",
            "Processed 226/286 batches\n",
            "Processed 228/286 batches\n",
            "Processed 230/286 batches\n",
            "Processed 232/286 batches\n",
            "Processed 234/286 batches\n",
            "Processed 236/286 batches\n",
            "Processed 238/286 batches\n",
            "Processed 240/286 batches\n",
            "Processed 242/286 batches\n",
            "Processed 244/286 batches\n",
            "Processed 246/286 batches\n",
            "Processed 248/286 batches\n",
            "Processed 250/286 batches\n",
            "Processed 252/286 batches\n",
            "Processed 254/286 batches\n",
            "Processed 256/286 batches\n",
            "Processed 258/286 batches\n",
            "Processed 260/286 batches\n",
            "Processed 262/286 batches\n",
            "Processed 264/286 batches\n",
            "Processed 266/286 batches\n",
            "Processed 268/286 batches\n",
            "Processed 270/286 batches\n",
            "Processed 272/286 batches\n",
            "Processed 274/286 batches\n",
            "Processed 276/286 batches\n",
            "Processed 278/286 batches\n",
            "Processed 280/286 batches\n",
            "Processed 282/286 batches\n",
            "Processed 284/286 batches\n",
            "Processed 286/286 batches\n",
            "\n",
            "Extracting CNN features from testing data...\n",
            "Processed 2/110 batches\n",
            "Processed 4/110 batches\n",
            "Processed 6/110 batches\n",
            "Processed 8/110 batches\n",
            "Processed 10/110 batches\n",
            "Processed 12/110 batches\n",
            "Processed 14/110 batches\n",
            "Processed 16/110 batches\n",
            "Processed 18/110 batches\n",
            "Processed 20/110 batches\n",
            "Processed 22/110 batches\n",
            "Processed 24/110 batches\n",
            "Processed 26/110 batches\n",
            "Processed 28/110 batches\n",
            "Processed 30/110 batches\n",
            "Processed 32/110 batches\n",
            "Processed 34/110 batches\n",
            "Processed 36/110 batches\n",
            "Processed 38/110 batches\n",
            "Processed 40/110 batches\n",
            "Processed 42/110 batches\n",
            "Processed 44/110 batches\n",
            "Processed 46/110 batches\n",
            "Processed 48/110 batches\n",
            "Processed 50/110 batches\n",
            "Processed 52/110 batches\n",
            "Processed 54/110 batches\n",
            "Processed 56/110 batches\n",
            "Processed 58/110 batches\n",
            "Processed 60/110 batches\n",
            "Processed 62/110 batches\n",
            "Processed 64/110 batches\n",
            "Processed 66/110 batches\n",
            "Processed 68/110 batches\n",
            "Processed 70/110 batches\n",
            "Processed 72/110 batches\n",
            "Processed 74/110 batches\n",
            "Processed 76/110 batches\n",
            "Processed 78/110 batches\n",
            "Processed 80/110 batches\n",
            "Processed 82/110 batches\n",
            "Processed 84/110 batches\n",
            "Processed 86/110 batches\n",
            "Processed 88/110 batches\n",
            "Processed 90/110 batches\n",
            "Processed 92/110 batches\n",
            "Processed 94/110 batches\n",
            "Processed 96/110 batches\n",
            "Processed 98/110 batches\n",
            "Processed 100/110 batches\n",
            "Processed 102/110 batches\n",
            "Processed 104/110 batches\n",
            "Processed 106/110 batches\n",
            "Processed 108/110 batches\n",
            "Processed 110/110 batches\n",
            "\n",
            "Preparing HAAR features...\n",
            "\n",
            "Feature Shapes:\n",
            "CNN Features Shape: (9123, 512)\n",
            "HAAR Features Shape: (9123, 4096)\n",
            "Epoch 1/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - accuracy: 0.6243 - loss: 1.4928 - val_accuracy: 0.8319 - val_loss: 1.1681 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.8076 - loss: 1.1501 - val_accuracy: 0.8651 - val_loss: 1.0266 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8546 - loss: 1.0060 - val_accuracy: 0.8651 - val_loss: 0.9849 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.8957 - loss: 0.8980 - val_accuracy: 0.9000 - val_loss: 0.8744 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9156 - loss: 0.8321 - val_accuracy: 0.9080 - val_loss: 0.8459 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9311 - loss: 0.7721 - val_accuracy: 0.9169 - val_loss: 0.8024 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.9380 - loss: 0.7303 - val_accuracy: 0.9186 - val_loss: 0.7849 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.9433 - loss: 0.7034 - val_accuracy: 0.9038 - val_loss: 0.8058 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.9511 - loss: 0.6707 - val_accuracy: 0.9055 - val_loss: 0.7849 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9530 - loss: 0.6491 - val_accuracy: 0.9115 - val_loss: 0.7579 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 30ms/step - accuracy: 0.9607 - loss: 0.6187 - val_accuracy: 0.9304 - val_loss: 0.6862 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.9635 - loss: 0.5992 - val_accuracy: 0.8926 - val_loss: 0.8007 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9606 - loss: 0.5873 - val_accuracy: 0.9161 - val_loss: 0.7043 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m285/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9658 - loss: 0.5647\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9658 - loss: 0.5647 - val_accuracy: 0.8963 - val_loss: 0.7672 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9705 - loss: 0.5426 - val_accuracy: 0.9035 - val_loss: 0.7399 - learning_rate: 2.5000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9731 - loss: 0.5260 - val_accuracy: 0.9135 - val_loss: 0.6923 - learning_rate: 2.5000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m285/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9770 - loss: 0.5138\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9770 - loss: 0.5138 - val_accuracy: 0.9135 - val_loss: 0.6876 - learning_rate: 2.5000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9771 - loss: 0.5052 - val_accuracy: 0.9009 - val_loss: 0.7310 - learning_rate: 1.2500e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9787 - loss: 0.5025 - val_accuracy: 0.9003 - val_loss: 0.7340 - learning_rate: 1.2500e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m284/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9795 - loss: 0.4932\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9795 - loss: 0.4932 - val_accuracy: 0.8952 - val_loss: 0.7464 - learning_rate: 1.2500e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9826 - loss: 0.4875 - val_accuracy: 0.9066 - val_loss: 0.6956 - learning_rate: 6.2500e-06\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 1 Accuracy: 0.9304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9632 - loss: 0.5996 - val_accuracy: 0.9063 - val_loss: 0.7545 - learning_rate: 1.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.9661 - loss: 0.5912 - val_accuracy: 0.9043 - val_loss: 0.7619 - learning_rate: 1.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.9639 - loss: 0.5870 - val_accuracy: 0.9055 - val_loss: 0.7653 - learning_rate: 1.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9669 - loss: 0.5766 - val_accuracy: 0.9112 - val_loss: 0.7374 - learning_rate: 1.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.9707 - loss: 0.5742 - val_accuracy: 0.9072 - val_loss: 0.7459 - learning_rate: 1.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9698 - loss: 0.5701 - val_accuracy: 0.9109 - val_loss: 0.7313 - learning_rate: 1.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9715 - loss: 0.5634 - val_accuracy: 0.8972 - val_loss: 0.7760 - learning_rate: 1.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.9753 - loss: 0.5515 - val_accuracy: 0.9012 - val_loss: 0.7486 - learning_rate: 1.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m284/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9718 - loss: 0.5557\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9718 - loss: 0.5556 - val_accuracy: 0.9055 - val_loss: 0.7452 - learning_rate: 1.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.9751 - loss: 0.5424 - val_accuracy: 0.8963 - val_loss: 0.7669 - learning_rate: 5.0000e-06\n",
            "Epoch 11/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9777 - loss: 0.5401 - val_accuracy: 0.8989 - val_loss: 0.7515 - learning_rate: 5.0000e-06\n",
            "Epoch 12/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9801 - loss: 0.5354\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9801 - loss: 0.5354 - val_accuracy: 0.8983 - val_loss: 0.7532 - learning_rate: 5.0000e-06\n",
            "Epoch 13/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9809 - loss: 0.5320 - val_accuracy: 0.9032 - val_loss: 0.7438 - learning_rate: 2.5000e-06\n",
            "Epoch 14/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9762 - loss: 0.5343 - val_accuracy: 0.8980 - val_loss: 0.7569 - learning_rate: 2.5000e-06\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "\n",
            "Stage 2 Accuracy: 0.9112\n",
            "Epoch 1/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9731 - loss: 0.5706 - val_accuracy: 0.9029 - val_loss: 0.7596 - learning_rate: 5.0000e-06\n",
            "Epoch 2/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9732 - loss: 0.5684 - val_accuracy: 0.9123 - val_loss: 0.7276 - learning_rate: 5.0000e-06\n",
            "Epoch 3/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.5664 - val_accuracy: 0.9060 - val_loss: 0.7499 - learning_rate: 5.0000e-06\n",
            "Epoch 4/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.9735 - loss: 0.5600 - val_accuracy: 0.9060 - val_loss: 0.7473 - learning_rate: 5.0000e-06\n",
            "Epoch 5/50\n",
            "\u001b[1m284/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9711 - loss: 0.5640\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9711 - loss: 0.5640 - val_accuracy: 0.9066 - val_loss: 0.7407 - learning_rate: 5.0000e-06\n",
            "Epoch 6/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9778 - loss: 0.5487 - val_accuracy: 0.9043 - val_loss: 0.7475 - learning_rate: 2.5000e-06\n",
            "Epoch 7/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.9744 - loss: 0.5500 - val_accuracy: 0.9095 - val_loss: 0.7311 - learning_rate: 2.5000e-06\n",
            "Epoch 8/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9718 - loss: 0.5529\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.9718 - loss: 0.5529 - val_accuracy: 0.9063 - val_loss: 0.7420 - learning_rate: 2.5000e-06\n",
            "Epoch 9/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.9762 - loss: 0.5505 - val_accuracy: 0.9058 - val_loss: 0.7481 - learning_rate: 1.2500e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9775 - loss: 0.5457 - val_accuracy: 0.9060 - val_loss: 0.7474 - learning_rate: 1.2500e-06\n",
            "Epoch 11/50\n",
            "\u001b[1m284/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9794 - loss: 0.5456\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.9794 - loss: 0.5456 - val_accuracy: 0.9066 - val_loss: 0.7443 - learning_rate: 1.2500e-06\n",
            "Epoch 12/50\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.9769 - loss: 0.5473 - val_accuracy: 0.9069 - val_loss: 0.7430 - learning_rate: 1.0000e-06\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 3 Accuracy: 0.9123\n",
            "\n",
            "Best Model Accuracy: 0.9304\n",
            "\n",
            "Saving models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Dropout, Input, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Path configurations\n",
        "DATA_PATH = {\n",
        "   'train_images': '/content/drive/MyDrive/Final Year Project Material/test',\n",
        "    'test_images': '/content/drive/MyDrive/Final Year Project Material/train',\n",
        "    'train_haar': '/content/drive/MyDrive/Final Year Project Material/NUUA_HAAR TRANSFORMED/haar_transformed_data_64x64-project-test.csv',\n",
        "    'test_haar': '/content/drive/MyDrive/Final Year Project Material/NUUA_HAAR TRANSFORMED/haar_transformed_data_64x64-project-train.csv'\n",
        "}\n",
        "\n",
        "# Constants\n",
        "IMG_SIZE = (299, 299)  # InceptionV3 requires 299x299 input\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2\n",
        "NUM_EPOCHS = 50\n",
        "VERBOSE = 1\n",
        "\n",
        "def setup_data_generators():\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function=tf.keras.applications.inception_v3.preprocess_input\n",
        "    )\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        DATA_PATH['train_images'],\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        preprocessing_function=tf.keras.applications.inception_v3.preprocess_input\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        DATA_PATH['test_images'],\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_generator, test_generator\n",
        "\n",
        "\n",
        "def load_haar_features():\n",
        "    train_haar_df = pd.read_csv(DATA_PATH['train_haar'])\n",
        "    test_haar_df = pd.read_csv(DATA_PATH['test_haar'])\n",
        "    return train_haar_df, test_haar_df\n",
        "\n",
        "\n",
        "def create_modified_inception_with_gradual_unfreezing():\n",
        "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE, 3))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "    return base_model, model\n",
        "\n",
        "\n",
        "def create_fusion_model(cnn_features_shape, haar_features_shape):\n",
        "    cnn_input = Input(shape=cnn_features_shape)\n",
        "    cnn_dense = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(cnn_input)\n",
        "    haar_input = Input(shape=haar_features_shape)\n",
        "    haar_dense = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(haar_input)\n",
        "\n",
        "    fusion = Concatenate()([cnn_dense, haar_dense])\n",
        "    x = Dense(256, activation='relu')(fusion)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=[cnn_input, haar_input], outputs=output)\n",
        "    return model\n",
        "\n",
        "def extract_cnn_features(model, generator):\n",
        "    features, labels = [], []\n",
        "    steps = len(generator)\n",
        "    for i in range(steps):\n",
        "        batch_x, batch_y = generator[i]\n",
        "        batch_features = model.predict(batch_x, verbose=0)\n",
        "        features.append(batch_features)\n",
        "        labels.append(batch_y)\n",
        "        if (i + 1) % 2 == 0:\n",
        "            print(f\"Processed {i + 1}/{steps} batches\")\n",
        "    return np.vstack(features), np.concatenate(labels)\n",
        "\n",
        "def prepare_haar_features(haar_df):\n",
        "    X = haar_df.drop('label', axis=1).values\n",
        "    y = haar_df['label'].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, y\n",
        "\n",
        "def gradual_unfreeze_and_train(base_model, fusion_model, train_cnn_features, train_haar_features,\n",
        "                                train_labels, test_cnn_features, test_haar_features, test_labels):\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # InceptionV3 has 311 layers compared to VGG16's 19 layers\n",
        "    # Adjust unfreezing strategy for InceptionV3\n",
        "    unfreeze_stages = [\n",
        "        (base_model.layers[-30:], 5e-5),        # Last 30 layers\n",
        "        (base_model.layers[-60:-30], 1e-5),     # Next 30 layers\n",
        "        (base_model.layers[-90:-60], 5e-6)      # Another 30 layers\n",
        "    ]\n",
        "\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for stage, (layers_to_unfreeze, learning_rate) in enumerate(unfreeze_stages, 1):\n",
        "        for layer in layers_to_unfreeze:\n",
        "            layer.trainable = True\n",
        "\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        fusion_model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        history = fusion_model.fit(\n",
        "            [train_cnn_features, train_haar_features],\n",
        "            train_labels,\n",
        "            validation_data=([test_cnn_features, test_haar_features], test_labels),\n",
        "            callbacks=[reduce_lr, early_stopping],\n",
        "            epochs=NUM_EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=VERBOSE\n",
        "        )\n",
        "\n",
        "        test_predictions = fusion_model.predict([test_cnn_features, test_haar_features])\n",
        "        predictions = (test_predictions > 0.5).astype(int).flatten()\n",
        "        current_accuracy = accuracy_score(test_labels, predictions)\n",
        "\n",
        "        print(f\"\\nStage {stage} Accuracy: {current_accuracy:.4f}\")\n",
        "\n",
        "        if current_accuracy > best_accuracy:\n",
        "            best_accuracy = current_accuracy\n",
        "            # First Save\n",
        "            fusion_model.save(f'/content/drive/MyDrive/Final Year Project Material/Suraj Code Copy/Best Fusion Model/InceptionV3_50_Epochs_64x64_HAAR_Stepwise_Unfreezing/best_fusion_model_stage_{stage}.h5')\n",
        "            # Second Save\n",
        "            fusion_model.save(f'/content/drive/MyDrive/Final Year Project Material/Save Models/Best Fusion Model/InceptionV3_50_Epochs_64x64_HAAR_Stepwise_Unfreezing/best_fusion_model_stage_{stage}.h5')\n",
        "\n",
        "    return fusion_model, best_accuracy\n",
        "\n",
        "def evaluate_model(model, test_cnn_features, test_haar_features, test_labels):\n",
        "    predictions = model.predict([test_cnn_features, test_haar_features])\n",
        "    predictions = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "    acc = accuracy_score(test_labels, predictions)\n",
        "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "    class_report = classification_report(test_labels, predictions)\n",
        "\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "    print('\\nConfusion Matrix:\\n', conf_matrix)\n",
        "    print('\\nClassification Report:\\n', class_report)\n",
        "\n",
        "    return acc, conf_matrix, class_report\n",
        "\n",
        "def train_fusion_model():\n",
        "    print(\"Loading data...\")\n",
        "    train_generator, test_generator = setup_data_generators()\n",
        "    train_haar_df, test_haar_df = load_haar_features()\n",
        "\n",
        "    print(\"\\nCreating and training models...\")\n",
        "    base_model, feature_extractor = create_modified_inception_with_gradual_unfreezing()\n",
        "\n",
        "    print(\"\\nExtracting CNN features from training data...\")\n",
        "    train_cnn_features, train_labels = extract_cnn_features(feature_extractor, train_generator)\n",
        "\n",
        "    print(\"\\nExtracting CNN features from testing data...\")\n",
        "    test_cnn_features, test_labels = extract_cnn_features(feature_extractor, test_generator)\n",
        "\n",
        "    print(\"\\nPreparing HAAR features...\")\n",
        "    train_haar_features, _ = prepare_haar_features(train_haar_df)\n",
        "    test_haar_features, _ = prepare_haar_features(test_haar_df)\n",
        "\n",
        "    print(\"\\nFeature Shapes:\")\n",
        "    print(\"CNN Features Shape:\", train_cnn_features.shape)\n",
        "    print(\"HAAR Features Shape:\", train_haar_features.shape)\n",
        "\n",
        "    fusion_model = create_fusion_model(train_cnn_features.shape[1:], train_haar_features.shape[1:])\n",
        "\n",
        "    final_model, best_accuracy = gradual_unfreeze_and_train(\n",
        "        base_model,\n",
        "        fusion_model,\n",
        "        train_cnn_features,\n",
        "        train_haar_features,\n",
        "        train_labels,\n",
        "        test_cnn_features,\n",
        "        test_haar_features,\n",
        "        test_labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\nBest Model Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    return final_model, None, feature_extractor\n",
        "\n",
        "def main():\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    for path_name, path in DATA_PATH.items():\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Path not found: {path_name} - {path}\")\n",
        "\n",
        "    fusion_model, history, feature_extractor = train_fusion_model()\n",
        "\n",
        "    print(\"\\nSaving models...\")\n",
        "\n",
        "    # First Save\n",
        "    feature_extractor.save('/content/drive/MyDrive/Final Year Project Material/Suraj Code Copy/InceptionV3_50_Epochs_64x64_HAAR_Stepwise_Unfreezing/feature_extractor_InceptionV3_50_epochs_64x64_haar_stepwise_unfreezing.h5')\n",
        "    fusion_model.save('/content/drive/MyDrive/Final Year Project Material/Suraj Code Copy/InceptionV3_50_Epochs_64x64_HAAR_Stepwise_Unfreezing/fusion_model_InceptionV3_50_epochs_64x64_haar_stepwise_unfreezing.h5')\n",
        "\n",
        "    # Second Save\n",
        "    feature_extractor.save('/content/drive/MyDrive/Final Year Project Material/Save Models/InceptionV3_50_Epochs_64x64_HAAR_Stepwise_Unfreezing/feature_extractor_InceptionV3_50_epochs_64x64_haar_stepwise_unfreezing.h5')\n",
        "    fusion_model.save('/content/drive/MyDrive/Final Year Project Material/Save Models/InceptionV3_50_Epochs_64x64_HAAR_Stepwise_Unfreezing/fusion_model_InceptionV3_50_epochs_64x64_haar_stepwise_unfreezing.h5')\n",
        "\n",
        "    return fusion_model, history, feature_extractor\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fusion_model, history, feature_extractor = main()"
      ]
    }
  ]
}